# ğŸ” Local Perplexity

Uma versÃ£o local do Perplexity AI otimizada para hardwares com 8GB de RAM, utilizando LLMs locais via Ollama.

## âœ¨ Features

- ğŸ§  **LLMs Locais**: Roda completamente offline usando Ollama
- âš¡ **Otimizado para 8GB RAM**: Performance otimizada para hardwares limitados  
- ğŸŒ **Busca Web Inteligente**: MÃºltiplas queries via Tavily API
- ğŸ¨ **Interface Dark Theme**: Visual idÃªntico ao Perplexity original
- ğŸ“ **FormataÃ§Ã£o Markdown**: Respostas estruturadas estilo Notion/ChatGPT
- ğŸ”„ **LangGraph Workflows**: OrquestraÃ§Ã£o avanÃ§ada de processos de IA
- ğŸ“± **UI Responsiva**: Interface limpa e moderna

## ğŸš€ Performance

- **Antes da otimizaÃ§Ã£o**: 5-10 minutos por resposta
- **ApÃ³s otimizaÃ§Ã£o**: ~30 segundos por resposta
- **Modelos suportados**: llama3.2:1b, llama3.2:3b
- **Consumo de RAM**: <4GB durante execuÃ§Ã£o

## ğŸ› ï¸ Stack TÃ©cnica

- **Frontend**: Streamlit
- **Backend**: Python 3.11+
- **LLM Framework**: LangChain + Ollama
- **Workflow**: LangGraph
- **Busca Web**: Tavily API
- **Modelagem**: Pydantic
- **Styling**: CSS customizado

## ğŸ“‹ PrÃ©-requisitos

- Python 3.11+
- Ollama instalado
- 8GB+ de RAM recomendado
- ConexÃ£o com internet (para buscas web)

## ğŸ”§ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio
```bash
git clone https://github.com/seu-usuario/local-perplexity.git
cd local-perplexity
```

### 2. Crie um ambiente virtual
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows
```

### 3. Instale as dependÃªncias
```bash
pip install -r requirements.txt
```

### 4. Configure o Ollama
```bash
# Instale o Ollama (https://ollama.ai)
# Baixe o modelo otimizado
ollama pull llama3.2:1b
```

### 5. Configure as variÃ¡veis de ambiente
```bash
cp .env.example .env
# Edite o .env com suas chaves de API
```

### 6. Execute a aplicaÃ§Ã£o
```bash
streamlit run perplexity_dark_theme.py
```

## âš™ï¸ ConfiguraÃ§Ã£o

### Arquivo .env
```env
TAVILY_API_KEY=sua_chave_tavily_aqui
OLLAMA_HOST=127.0.0.1:11434
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_NUM_PARALLEL=1
```

### Obtendo Chave Tavily API
1. Acesse [tavily.com](https://tavily.com)
2. Crie uma conta gratuita
3. Gere sua API key
4. Adicione no arquivo `.env`

## ğŸ¯ Como Usar

1. **Inicie a aplicaÃ§Ã£o**: `streamlit run perplexity_dark_theme.py`
2. **Acesse no browser**: `http://localhost:8501`
3. **Digite sua pergunta** no campo de busca
4. **Clique em Search** e aguarde a resposta
5. **Visualize fontes** na seÃ§Ã£o Sources

## ğŸ“ Estrutura do Projeto

```
local-perplexity/
â”œâ”€â”€ perplexity_dark_theme.py    # AplicaÃ§Ã£o principal
â”œâ”€â”€ schemas.py                  # Modelos Pydantic
â”œâ”€â”€ utils.py                   # FunÃ§Ãµes de busca
â”œâ”€â”€ prompts.py                 # Templates de prompts
â”œâ”€â”€ requirements.txt           # DependÃªncias Python
â”œâ”€â”€ .env.example              # Exemplo de configuraÃ§Ã£o
â”œâ”€â”€ .gitignore               # Arquivos ignorados
â””â”€â”€ README.md               # Este arquivo
```

## ğŸ”„ Workflows

O sistema utiliza LangGraph para orquestrar o seguinte workflow:

1. **Generate Queries**: Gera 2-3 queries de busca otimizadas
2. **Execute Search**: Busca informaÃ§Ãµes via Tavily API
3. **Write Response**: Sintetiza resposta usando LLM local

```mermaid
graph LR
    A[User Input] --> B[Generate Queries]
    B --> C[Execute Search]
    C --> D[Write Response]
    D --> E[Display Result]
```

## âš¡ OtimizaÃ§Ãµes para 8GB RAM

- **Modelo leve**: llama3.2:1b (~1.3GB)
- **Context window reduzido**: 3072 tokens
- **Queries limitadas**: MÃ¡ximo 3 por busca
- **Cache de recursos**: Streamlit cache para LLM
- **Busca paralela**: ThreadPoolExecutor otimizado
- **Timeouts configurados**: Evita travamentos

## ğŸ› Troubleshooting

### Problema: Ollama nÃ£o encontrado
```bash
# Verifique se o Ollama estÃ¡ rodando
ollama list

# Se nÃ£o estiver, inicie o serviÃ§o
ollama serve
```

### Problema: MemÃ³ria insuficiente
```bash
# Use modelo menor
ollama pull llama3.2:1b

# Configure variÃ¡veis de ambiente
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_NUM_PARALLEL=1
```

### Problema: Timeout nas buscas
- Verifique sua conexÃ£o de internet
- Confirme se a chave Tavily API estÃ¡ correta
- Reduza o nÃºmero de queries no cÃ³digo

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. Commit suas mudanÃ§as (`git commit -am 'Adiciona nova feature'`)
4. Push para a branch (`git push origin feature/nova-feature`)
5. Abra um Pull Request

## ğŸ“ TODO

- [ ] Sistema de cache para respostas
- [ ] HistÃ³rico de conversas
- [ ] Suporte a mÃºltiplos modelos
- [ ] Export de respostas em PDF
- [ ] Modo offline completo
- [ ] Interface web responsiva
- [ ] IntegraÃ§Ã£o com mais APIs de busca

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## ğŸ™ CrÃ©ditos

- **Ideia original**: [Asimov](https://asimov.academy)
- **OtimizaÃ§Ãµes e melhorias**: Lucas Rosati Cavalcanti
- **InspiraÃ§Ã£o**: Perplexity AI

## ğŸ“ Contato

- LinkedIn: [Lucas Rosati](https://www.linkedin.com/in/lucas-rosati-cavalcanti-pereira-b62229128/))
- GitHub: [@lucasrosati](https://github.com/lucasrosati)

---

â­ Se este projeto te ajudou, considere dar uma estrela no repositÃ³rio!
